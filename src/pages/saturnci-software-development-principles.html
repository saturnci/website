---
page_title: SaturnCI software development principles
nav: saturnci-software-development-principles
draft: true
---

<div class="container page-container">
  <h1>SaturnCI software development principles</h1>

  <div class="page-content">
    <p>
      As soon as I discovered Claude Code, I became an enthusiastic user.
      Having an AI agent which I interact with on my terminal, which is aware
      of all my code, and which can edit files and execute commands on my
      behalf, is so much better than switching between my editor and the
      browser and copy/pasting code and responses. To me the old way of
      interacting with AI now seems comically quaint.
    </p>

    <p>
      Even though Claude Code is in some ways spectacularly impressive, in
      other ways it's frustratingly stupid.
    </p>

    <h2>Stay in touch with reality</h2>
    <p>
      Time is finite and precious. To blow one's limited budget on work which
      is mistakenly believed to be impactful, but is actually not, is a grave
      error. A clear and thorough understanding of the customer's world is a
      prerequisite to judging the merits of any prospective project. Such
      understanding is gained not mainly through telemetrics or "data" (a
      catch-all word whose meaning has been diluted to the point of near
      uselessness), but through face-to-face conversations with real people.
      Qualitative information is almost always more valuable than quantitative
      data.
    </p>

    <h2>Favor safe bets</h2>
    <p>
      Every software organization always has a long list of tasks and projects
      that they know for absolute certain need to be done. This work is usually
      boring, like fixing a bug or performing an upgrade. There is also always
      a constant flow of fun and exciting ideas which <i>might</i> yield a
      positive return on investment but also might not. There are of course
      exceptions, but in general, it's smarter to favor the safe bets over the
      riskier ones. It's better to work for a paycheck than to wait to win the
      lottery.
    </p>

    <h2>No speculation</h2>
    <p>
      This is very closely related to the "favor safe bets" principle. What I
      define as speculation is a feature or a piece of code which is not only
      not a safe bet but which has <i>no</i> concrete justification for
      existing. A classic example of speculative code is a parameter which has
      a default value but which will never be missing.
    </p>

    <p>
      The value of speculative code is less than zero. It's all liability and
      no benefit. Not only does speculative code add accidental complexity and
      possible confusion to a system but the time spent writing the speculative
      code incurs an opportunity cost since that time could have been used to
      build something valuable instead.
    </p>

    <h2>Have high standards</h2>
    <p>
      To build something excellent is more fun and satisfying than to build something mediocre.
    </p>

    <h2>Have a high tolerance for mistakes, but correct them diligently</h2>
    <p>
      Don't be afraid to make mistakes. Mistakes are inevitable. The question
      isn't how good you are at avoiding mistakes (which is impossible anyway),
      it's how good you are at detecting and correcting them.
    </p>

    <h2>Fall in love with the customer</h2>
    <p>
      To paraphrase Jay Abraham, don't fall in love with your business, don't
      fall in love with your product, fall in love with your customer. If you
      treat the business or the product as the central entity, then your
      business activities are liable to drift away from what customers are
      actually interested in, and business will likely decline.
    </p>

    <h2>Maintain high usability</h2>
    <p>
      Software isn't good unless it's easy to use.
    </p>

    <h2>Maintain conceptual soundness and integrity</h2>

    <h2>No jank</h2>
    <p>
      There are a few software behaviors that I encounter over and over which I
      positively despise. One is when a page loads in a "popcorn" fashion,
      causing the text I'm reading to jerk up or down while I'm reading it,
      causing me to lose my place.
    </p>

    <p>
      Another behavior I hate is when a form field starts validating my input
      not when I submit the form or when I blur the field, but <i>when I start
      typing</i>. I type the first digit of my phone number and it complains
      that that single digit is not a valid phone number. Well, no shit it's not
      a valid phone number. How about you wait until I stop typing in the field
      before you try to validate it? Such behavior makes the form feel
      half-broken.
    </p>

    <p>
      Pagination is not a new or complicated kind of feature, yet some sites
      manage to fuck it up spectacularly. My biggest pet peeve is when I scroll
      to the bottom of one page, then click the button to advance to the next
      page, and it does so, but keeps me scrolled to the bottom of the page.
      What did you think, that I wanted to read page 1 from the top down but
      page 2 from the bottom up or something? Such behavior makes the feature
      feel half-baked and ill-thought-out. I will never not be mad about it.
    </p>

    <p>
      And then there are some things which are merely not good: unstyled
      flashes when a page loads, blurry/aliased images, buttons which take a
      little too long to respond.
      All these things are <i>jank</i>. In SaturnCI I want none of it.
    </p>

    <h2>Don't tolerate defects</h2>
    <p>
      One of the twelve questions on the famous
      <a href="https://www.joelonsoftware.com/2000/08/09/the-joel-test-12-steps-to-better-code/">Joel Test</a>
      (a rough checklist to evaluate the quality of a software team) is "Do you
      fix bugs before writing new code?" If we don't fix bugs before writing
      new code, then we're building on a shaky foundation, and the bugs are
      liable to multiply.
    </p>

    <p>
      But actually, the rule of thumb "fix bugs before writing new code" is a
      bit overly simplistic. There are three types of <i>defects</i> in
      software of which bugs are only one. The other two types are <i>design
      defects</i> and <i>missing features</i>. Often times a design defect or a
      missing feature constitutes a worse problem than a bug. For example, at the
      time of this writing, SaturnCI has a small bug where users are
      illogically allowed to add the same repository twice. This behavior is
      obviously wrong but it's not a showstopper. At the same time, there's a
      missing feature where users have no way to create API authentication
      tokens; the only way one can be created is for me personally to manually
      create in in the database, meaning that no user can use the API (or
      rotate their tokens) without manual work on my part. Obviously, the
      missing API token creation feature constitutes a worse problem than the
      duplicate repository bug, and I'm certainly going to give the token issue
      priority. So my rule is: "in general, fix defects before writing new
      code".
    </p>

    <p>
      <i>Side note: from here on out, when I say "bug", I mean "bug worth
      fixing". Every system has many small bugs which will never be
      worth fixing. For our purposes we can ignore these.</i>
    </p>

    <p>
      Another question in The Joel Test is "Do you have a bug database?"
      I don't really like the sound of this idea. Obviously it's a good idea for
      software teams to use some sort of issue tracking for bugs and features,
      but the term <i>bug database</i> to me implies not just a dozen or two
      bugs but a lot.
    </p>

    <p>
      Every program has bugs. You could say that every program has a
      <i>queue</i> of bugs. New bugs are constantly arriving and existing bugs
      are constantly getting fixed. If new bugs arise at a faster rate than
      they get fixed, then the queue will grow indefinitely. If bugs get fixed
      at a faster rate than they arise, then the queue will shrink to a
      definite size and never grow larger. In my experience, most organizations
      have a bug queue of the always-growing type. When this is the case, it's
      a mathematical necessity that <i>most</i> of the bugs in the queue will
      <i>never</i> get fixed. If a team maintains a backlog of bugs, most of
      which will never get fixed, then what's the point of the backlog? The
      only function it serves is to help maintain the self-deception that the
      bugs will all get fixed at some point. But they will not.
    </p>

    <p>
      The only way to keep the queue of bugs (or, to use my preferred approach,
      defects) from growing indefinitely is to fix defects at a faster rate
      than they appear. This is a hard law of reality. There is no other way.
      In my experience this means spending a lot of time fixing defects, often
      more than what might seem "reasonable". But if keeping the defect queue
      from growing requires an unreasonable time investment, it does not
      logically follow that making only a "reasonable" time investment in
      fixing bugs is a smarter approach. Still, this is the approach that most
      organizations take. New changes "need" to be made, and so existing
      defects go unfixed, with the overall effect being that development slows
      as the system grows more fragile and convoluted, to the point where,
      eventually, it's excruciatingly hard to make any meaningful change to the
      system at all. I prefer to just fix the damn defects.
    </p>

    <h2>No epicycles</h2>
    <p>
      In ancient times, before astronomers figured out than the Sun rather than
      the Earth is at the center of the solar system, astronomers tried to
      figure out how to model the motions of the planets under the premise of
      geocentrism. Because their geocentric model of the solar system was
      fundamentally wrong, the models they came up with for planetary motion
      weren't simple and elegant like Newton's heliocentric, elliptical models
      are; they were complicated and convoluted. You can see a wonderful video
      illustration of the difference between heliocentric and geocentric
      planetary motion
      <a href="https://www.youtube.com/watch?v=ZeS8h1t-uMA">here</a>.
    </p>

    <p>
      From the perspective of Earth, the motion of a planet can be roughly (or
      extremely roughly, depending on the planet) approximated by a circle. But
      there's a problem in that some planets appear to occasionally reverse
      direction for a bit, then reverse direction again, a phenomenon called
      retrograde motion. Some clever ancient astronomers figured out that
      retrograde motion could be accounted for by putting smaller circular
      orbits on top of the bigger circular orbits, as though each planet were
      a moon orbiting around some other, invisible planet. These smaller orbits
      were called
      <a href="https://en.wikipedia.org/wiki/Deferent_and_epicycle">epicycles</a>.
    </p>

    <p>
      Epicycles actually predicted the positions of planets quite well. So in
      that sense I suppose they were a good answer to the problem. But of
      course, they were a fudge. They existed solely to compensate for the
      fundamental unsoundness of the geocentric model. Once the heliocentric
      model was discovered, a dramatically simpler model of planetary motion
      snapped into place.
    </p>

    <p>
      Many software projects are full of metaphorical epicycles. Instead of
      fixing a fundamental defect, the developers "add an epicycle" to paper
      over the defect's weaknesses. Often, the epicycles end up needing
      epicycles of their own. What could have been modeled simply ends up being
      modeled in a very convoluted way. Adding to the problem is that unlike
      science, which is self-correcting by nature, software tends to be what
      you might call self-ossifying. Once a poor model gets a layer or two of
      epicycles piled on top of it, the weight of the epicycles is so great
      that the model is unable to change. Because of the grave risk that
      epicycles pose, they should be avoided completely.
    </p>

    <h2>Don't cut with dull blades</h2>
    <p>
      Ancient astronomers had their broken model of geocentrism. Modern
      software developers have a broken model called technical debt. One of the
      biggest weaknesses of the technical debt metaphor is that it allows the
      belief that technical debt can be taken on "strategically", the way that
      financial debt can. The reality is that "strategic" technical debt gets
      paid back approximately never. Another big flaw with the technical debt
      metaphor is that unlike financial debt which can be taken on or avoided
      by choice, technical debt accumulates naturally, by default. So-called
      "technical debt" isn't debt, it's something else. What is it?
    </p>

    <p>
      The fact that software gets worse over time by default, and that it takes
      a lot of careful work to reverse the deterioration, suggests that this
      deterioration phenomenon has something to do with <i>entropy</i>, the
      tendency for the amount of disorder in a system to increase over time.
      There are many more ways for things to go wrong than right. If you
      completely disassemble a car, there are infinite ways to reassemble the
      pieces that would <i>not</i> result in a working automobile, and just a
      few that would. Since entropy is a fundamental aspect of reality and acts
      on everything all the time (tires and brake pads wear down, oil gets
      dirty, weeds grow in gardens, old people get dementia, security exploits
      are found in libraries, etc.), constant work is needed in order to stave
      off entropy's deleterious effects. Software is subject to entropy not
      metaphorically but quite literally. However, since entropy is far from a
      universally-understood concept, we would benefit from a metaphor that's a
      bit more easily graspable to the average person.
    </p>

    <p>
      Blades&mdash;kitchen knives, saw blades, etc.&mdash;are subject to
      entropy just like everything else. Unless entropy is constantly combatted
      via sharpening, the blades will get increasingly dull until they're quite
      unsuitable for their purpose. Not only is a dull blade less effective and
      efficient than a sharp one, it's more dangerous.
    </p>

    <p>
      Software is the same. Unless entropy is constantly combatted via
      refactoring, the system will get increasingly hard to understand and
      change. Each area of a software system (any way you choose to slice it
      up) can be thought of as an individual blade. An area that's easy to
      change is a sharp blade; if it's hard to change then it's a dull blade.
      In general, the more a certain area gets changed without refactoring,
      the worse the code gets. The blade dulls with use.
    </p>

    <p>
      In general, it's not a good idea to cut with dull blades. If you start
      using a blade and you discover that it's dull, it's almost certainly
      worthwhile to spend a bit of time making the blade sharper, at least
      sharp enough so your current cutting task isn't miserable. Or perhaps you
      find, after using an originally sharp blade, that it is now dull. That
      also tends to be a good time to do some sharpening.
    </p>

    <h2>Don't sharpen blades speculatively</h2>
    <p>
      That it's better to cut with sharp blades than dull ones does not mean
      that every possible act of sharpening is worthwhile. Every act of
      sharpening is an investment and a bet. When you sharpen a blade, you're
      saying, "I'm investing in sharpening this blade now because I <i>bet</i>
      the investment will pay off in the future." (Note carefully that you
      can't know in advance whether any particular bet will pay off, and to try
      to only make 100% sure bets and never lose bets is a fool's errand. The
      goal is for the investment portfolio as a whole to yield a positive
      return on investment.)
    </p>

    <p>
      Sharpening a blade right before you use it is an excellent bet. The
      payoff will be realized within hours or minutes! Sharpening a blade right
      after using it can also make for a very good bet, since you've just
      experienced that the blade is dull and you have good evidence that this
      blade is in fact one that sometimes gets used.
    </p>

    <p>
      What's generally never a good idea is to grab some blade that's just
      lying around and sharpen it speculatively, on the expectation that that
      blade <i>might</i> be used someday. Doing so incurs an opportunity cost.
      The price you paid to sharpen this idle blade could have gone toward
      making a surer investment in sharpening a more heavily-used blade, or it
      could have gone toward cutting. A blade that's never used does no harm in
      being dull.
    </p>

    <h2>The authoritative source of truth for the system's behavior is not the application code but the test code</h2>
    <p>
      In a system with no tests, the application code does two jobs: it both a)
      determines the behavior of the system and b) serves as the authoritative
      specifications the system. Conversely, in a system with a complete test
      suite, the application code still determines the behavior of the system
      but it doesn't serve as the system's specifications. That role is now
      filled by the tests.
    </p>

    <p>
      Why should we care about that? What's so great about having the roles of
      specification and behavior played by two different entities? If the
      application code is free from the job of having to keep track of the
      specifications, then the code can be changed freely without disturbing
      the specifications one bit. There's no longer pressure on the application
      code to be correct. The tests are taking care of that. The application
      code is now less sacred and precious. If the code becomes incorrect, it's
      not necessarily a very big deal, because the job of worrying about the
      correctness of the system's behavior can be left to the tests.
    </p>

    <p>
      Working this way takes the pressure off the application code, but the
      pressure has not been removed, merely shifted from the application code
      to the tests. Now, since the behavior specified by the tests is what's
      being enforced, it's critical that the specified behavior matches the
      actual desired behavior.
    </p>

    <h2>Work on one thing at a time</h2>
    <p>
      The fastest way to get a bunch of things done is to do one thing at a
      time. Obviously, no one can literally do more than one piece of work at a
      time. I've never seen someone sitting behind two laptops, typing on one
      with their left hand and the other one with their right hand. When
      developers multitask, they do so by switching back and forth between two
      tasks (or, God forbid, three or more). Maybe they switch to task B while
      they're waiting for a test run on task A or a deployment. This way, time
      that would otherwise be idle doesn't go to waste.
    </p>

    <p>
      There are exceptions, but in general I don't think this way of working is
      a net win. Personally, when I switch between two programming tasks, I
      tend to find myself forgetting which one is the "primary" task and which
      one is secondary. Perhaps I started task B mainly to fill time during
      idle time of task A, even though task A is much more essential than B.
      But then I inevitably find myself focusing on task B, at times, at the
      expense of task A.
    </p>

    <p>
      The implicit belief behind filling idle time is that idle time is
      <i>bad</i>. The ideal utilization rate (according to this belief) is
      100%, and if my utilization rate is 80%, then the remaining 20% needs to
      be filled up with some sort of work or else that time is wasted. The
      aim is, more or less, perfect efficiency.
    </p>

    <p>
      But as a general law of reality, perfect efficiency is not possible. If
      you have a car wash or a restaurant or a ticket booth, there's always
      either a line of people waiting or at least one idle worker. It would be
      ideal to always have all workers fully utilized so you're getting your
      money's worth, while also never making any customer wait in line, but
      that's not possible. Either there are workers "wasting" their time being
      idle, or customers "wasting" their time waiting in line. Waste is
      inevitable.
    </p>

    <p>
      A developer switching between two tasks&mdash;I'll call it "serving" two
      tasks&mdash;is like a waiter serving two tables. Any time the waiter is
      serving one table, he is unavailable to serve the other. If someone at
      table A needs another gin and tonic but the waiter is currently occupied
      with table B, the customer at table A will have to wait. If the waiter
      were serving table A and table A only, then the waiter would be
      "wastefully" idle some portion of the time, but any need from table A
      could be served instantly.
    </p>

    <p>
      Waiting on a test run or a deployment or what have you is a bit like
      waiting for a customer to finish his food or drink. At some
      semi-unpredictable time, the "work" will finish and your service will be
      needed. If you go off and occupy yourself with something else to fill
      the idle time, then there's a good chance you won't be available right
      when you're needed again, and your "customer" will have to wait.
    </p>

    <p>
      When all the customers are equally important, this is not a big deal, and
      in fact, in real restaurants (except perhaps the fanciest ones), waiters
      do of course divide themselves among multiple tables. But what if some
      customers are higher priority than others? Let's say you're a waiter
      serving exactly two tables. At one table is the President of the United
      States (let's leave aside <i>which</i> president) and at the other table
      is someone very unimportant; let's call him Larry. We want to serve the
      president's every whim immediately, but Larry can go to hell for all we
      care. This makes it irrational to try to fill "wasted time" by serving
      Larry while we're not serving the president. We should have exactly two
      modes: 1) actively serving the president and 2) attentively waiting for
      the president's next need. If we make the president wait because we were
      too busy serving Larry, we have failed.
    </p>

    <p>
      An engineering organization doesn't exist for the purpose of keeping all
      its employees fully utilized at all times. It exists to <i>get work
      done</i>. The highest-priority work in an engineering organization is
      metaphorically the president; all the other work is Larry. Just as it
      doesn't make sense to serve Larry at the expense of the president, it
      doesn't make sense to serve lower-priority work at the expense of
      top-priority work. It might feel "wrong" for a programmer to be idle, in
      a way that it doesn't necessarily feel wrong for a waiter, but that
      feeling is just our unsophisticated gut instincts talking. In order to
      achieve global efficiency, we have to be comfortable with a certain
      amount of local inefficiency. Make peace with the waste. Work on one
      thing at a time.
    </p>

    <h2>Don't mix jobs</h2>

    <h2>Minimize work-in-process</h2>

    <p>
      Nothing has value until it's running in production. Changes that have
      been started but aren't running in production yet are changes we've paid
      for but can't benefit from yet. Why is that bad? Consider a bakery which

    </p>

    <h2>Keep everything working all the time</h2>

    <h2>Keep all work atomic</h2>

    <h2>Cut scope to the bone</h2>

    <h2>Minimize third-party dependencies</h2>

    <h2>Maintain exceptionally good performance</h2>

    <h2>Test at as low a level as possible</h2>
  </div>
</div>
